{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from multiprocessing.dummy import Pool, Lock, Value\n",
    "from time import sleep\n",
    "from requests.exceptions import ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url, n_attempts=5, t_sleep=2, exc_counter=0):\n",
    "    try:\n",
    "        for i in range(n_attempts):\n",
    "            resp = req.get(url)\n",
    "            if not resp:\n",
    "                sleep(t_sleep)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        return resp\n",
    "    except ConnectionError as e: # in case if website timeouted me\n",
    "        print(url, ' ', e)\n",
    "        exc_counter += 1\n",
    "        sleep(t_sleep + 1)\n",
    "        if exc_counter < n_attempts:\n",
    "            get_page(url, err_counter=exc_counter)\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutex = Lock()\n",
    "\n",
    "def parse_page(url, book_list):\n",
    "    resp = get_page(url)\n",
    "    if not resp:\n",
    "        print('couldn\\'t get page with list of books ', url)\n",
    "        return\n",
    "    soup0 = BeautifulSoup(resp.text, 'lxml')\n",
    "    books = soup0.find('div', class_=\"rd-page-listing__products\").\\\n",
    "                            find_all('div', class_=\"rd-listing-product-item-data-wrap\")\n",
    "    next_p_button = soup0.find('a', class_=\"pagination-next\")\n",
    "    for i in books:\n",
    "        book_list.append(i)\n",
    "        with mutex:\n",
    "            global n_processed_links\n",
    "            n_processed_links.value += 1\n",
    "            print(f\"\\r{n_processed_links.value} links are processed...\", end='', flush=True)\n",
    "    return next_p_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_book(book_url):\n",
    "    url = template_link + book_url.find('a').get('href')\n",
    "    resp = get_page(url)\n",
    "    if not resp:\n",
    "        print('couldn\\'t get page of book ', url)\n",
    "        return\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    this_page_dict = {\n",
    "        'ID': soup.find('span', itemprop=\"sku\").text,\n",
    "        'Название': soup.find('h1', class_=\"rd-page-product__title\").text,\n",
    "        'Автор': soup.find('a', itemprop=\"brand\").text,\n",
    "        'Категория': '; '.join(map(lambda t: t.text.strip(),soup.find('div', class_=\"rd-page-product__breadcrumbs\").\\\n",
    "                                   find_all('span', itemprop='name'))),\n",
    "        'Изображение': soup.find('meta', property=\"og:image\").get('content'),\n",
    "        'Наличие': soup.find('span', class_=\"rd-page-product__buy-text\").text == 'Купить',\n",
    "    }\n",
    "    desc = soup.find('div', class_=\"rd-page-product__desc-body\").find_all('p')\n",
    "    if desc:\n",
    "        this_page_dict['Описание'] = ' '.join(map(lambda t: t.text.strip(), desc))\n",
    "    else:\n",
    "        this_page_dict['Описание'] = soup.find('div', class_=\"rd-page-product__desc-body\", itemprop=\"description\").text.strip()\n",
    "    characteristics = soup.find('div', class_=\"rd-page-product__desc-params\").find_all('p')\n",
    "    for i in characteristics:\n",
    "        this_page_dict[i.find(itemprop=\"name\").text] = i.find(itemprop=\"value\").text\n",
    "\n",
    "    price = soup.find('div', itemtype=\"http://schema.org/Offer\")\n",
    "    this_page_dict['Цена'] = price.find('span', class_=\"num\").text\n",
    "    old_price = price.find('span', class_=\"prev\")\n",
    "    if old_price:\n",
    "        this_page_dict['Цена (старая)'] = old_price.text.strip().split()[0]\n",
    "    feedback = soup.find('div', class_=\"rd-rating-stars\")\n",
    "    if feedback.find('span', itemprop=\"aggregateRating\"):\n",
    "        this_page_dict['Число отзывов'] = feedback.find('meta', itemprop=\"reviewCount\").get('content')\n",
    "        this_page_dict['Число оценок'] = feedback.find('meta', itemprop=\"ratingCount\").get('content')\n",
    "        this_page_dict['Оценка'] = feedback.find('meta', itemprop=\"ratingValue\").get('content')\n",
    "\n",
    "    preview = soup.find('a', class_=\"download-pdf\")\n",
    "    if preview:\n",
    "        this_page_dict['Превью'] = template_link + preview.get('href')\n",
    "    res_list.append(this_page_dict)\n",
    "    with mutex:\n",
    "        global n_processed_books\n",
    "        n_processed_books.value += 1\n",
    "        print(f\"\\r{n_processed_books.value} books are processed...\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(author):\n",
    "    url = 'https://www.respublica.ru/authors/{}'.format(author)\n",
    "    next_p_button = parse_page(url, book_list)\n",
    "    while next_p_button:\n",
    "        next_p_url = template_link + next_p_button.get('href')\n",
    "        next_p_button = parse_page(next_p_url, book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2453 links are processed..."
     ]
    }
   ],
   "source": [
    "template_link = 'https://www.respublica.ru'\n",
    "book_list = []\n",
    "n_processed_links = Value('i', 0)\n",
    "with open('./Data_analysis/Site_parsing_hw/authors.txt', 'r') as f:\n",
    "    with Pool(5) as pool:\n",
    "        pool.map(get_all_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2453"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2453 books are processed..."
     ]
    }
   ],
   "source": [
    "#parsing all books from the list we got\n",
    "res_list = []\n",
    "n_processed_books = Value('i', 0)\n",
    "with Pool(8) as pool:\n",
    "    pool.map(parse_book, book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2453"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res_list).fillna('')\n",
    "df.sort_values(by=['ID'], inplace=True)\n",
    "with open('./hw_3.csv', mode='w', encoding = 'utf-8') as f_csv:\n",
    "    tmp = df.to_csv(index=False, sep='\\t')\n",
    "    f_csv.write(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
